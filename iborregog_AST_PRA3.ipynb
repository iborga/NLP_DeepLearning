{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.977 · Anàlisi de sentiments i textos</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Màster universitari en Ciències de dades (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudi d'Informàtica, Multimèdia i Telecomunicacions</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRA 3: Deep Learning per a l'anàlisi de textos i sentiments\n",
    "\n",
    "En aquesta pràctica revisarem i aplicarem els coneixements apresos en els mòduls 4 i 5. Tractarem els següents temes:\n",
    "\n",
    "1. ** Traducció automàtica **: amb custom embeddings i amb embeddings preentrenats.\n",
    "2. ** Classificació de frases **: Aplicació dels conceptes treballats per a la reutilització de l'arquitectura de dos models.\n",
    "3. ** Anàlisi de sentiments **: anàlisi de sentiments de textos.\n",
    "4. ** Anàlisi de sentiments per aspectes (ABSA) **: anàlisi d'aspectes en comentaris.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Així com altres temes transversals treballats al llarg de l'assignatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta primera part de la pràctica es demana resoldre els exercici amb la llibreria **Keras**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Traducció Automàtica (3 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 TA amb Custom Embeddings (2 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "L'objectiu d'aquest apartat és entrenar un model de traducció automàtica entre anglès i alemany, seguint els mateixos passos que en el notebook de Machine Translation del mòdul 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Implementació:</strong> Seguint els passos treballats en el notebook de traducció automàtica, i partint de les mateixes dades, implementa i entrena un model de traducció automàtica, aquesta vegada de l'anglès a l'alemany. <br>\n",
    "    - La capa embedding ha de tenir dimensió igual a 300 <br>\n",
    "    - es recomana una longitud màxima de seqüència de 12 <br>\n",
    "    - es recomana usar els primers 50000 parells de el corpus deu.txt <br>\n",
    "    \n",
    "Mostra l'aplicació del model entrenat amb algun exemple.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓ                                   #\n",
    "#############################################\n",
    "from numpy import array\n",
    "\n",
    "# Lectura del text\n",
    "def read_text(filename):\n",
    "        file = open(filename, mode='rt', encoding='utf-8')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "\n",
    "# Text a parells angles-alemany\n",
    "def to_lines(text):\n",
    "    sents = text.strip().split('\\n')\n",
    "    sents = [i.split('\\t')[:2] for i in sents]\n",
    "    return sents    \n",
    "\n",
    "data = read_text(\"deu.txt\")\n",
    "eng_deu = to_lines(data)\n",
    "eng_deu = array(eng_deu)\n",
    "\n",
    "eng_deu = eng_deu[:50000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi' 'hallo']\n",
      " ['hi' 'grüß gott']\n",
      " ['run' 'lauf']\n",
      " ...\n",
      " ['i wholeheartedly agree' 'ich stimme rückhaltlos zu']\n",
      " ['i will always love you' 'ich werde dich immer lieben']\n",
      " ['i will be back by nine' 'um neun bin ich wieder zurück']]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessament\n",
    "import string\n",
    "\n",
    "# sense signes de puntuació\n",
    "eng_deu[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_deu[:,0]]\n",
    "eng_deu[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_deu[:,1]]\n",
    "\n",
    "# tot en minúscules\n",
    "for i in range(len(eng_deu)):\n",
    "    eng_deu[i,0] = eng_deu[i,0].lower()\n",
    "    eng_deu[i,1] = eng_deu[i,1].lower()\n",
    "\n",
    "print(eng_deu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6361\n",
      "Deutch Vocabulary Size: 10597\n"
     ]
    }
   ],
   "source": [
    "# Creació del vocabulari amb parelles paraula-index\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenization(sentences):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer\n",
    "\n",
    "engtok= tokenization(eng_deu[:,0])\n",
    "deutok= tokenization(eng_deu[:,1])\n",
    "\n",
    "engvsize= len(engtok.word_index)+1\n",
    "deuvsize= len(deutok.word_index)+1\n",
    "print('English Vocabulary Size: %d'%engvsize)\n",
    "print('Deutch Vocabulary Size: %d'%deuvsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Conjunts de train i test\n",
    "train, test = train_test_split(eng_deu, test_size=0.2, random_state=12)\n",
    "\n",
    "# Codificació de les seqüencies amb els index de les paraules, i padding fins longitud\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq\n",
    "\n",
    "seql=12\n",
    "trainX = encode_sequences(engtok, seql, train[:,0])\n",
    "trainY = encode_sequences(deutok, seql, train[:,1])\n",
    "testX = encode_sequences(engtok, seql, test[:,0])\n",
    "testY = encode_sequences(deutok, seql, test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "\n",
    "# El model és sequencial i consta d'un encoder i un decoder\n",
    "def encodeco(in_vocab_size, vec_length, max_text_length, out_timesteps, out_vocab_size):\n",
    "    mt_model = Sequential()\n",
    "    # Encoder\n",
    "    # La capa embedding se inicializa con pesos aleatorios y aprende una representación vectorial (embeddings) \n",
    "    # para todas las palabras de las frases. Los embeddings podrian ser también vectores Word2Vec o Glove precalculados\n",
    "\n",
    "    # Es necesario tener: \n",
    "    # el tamaño del vocabulario de los textos (vocab_size)\n",
    "    # la dimensión de los vectores en los que las palabras serán embedded (embedding_vec_length).\n",
    "    # la longitud máxima de las secuencias de input\n",
    "    mt_model.add(Embedding(in_vocab_size, vec_length, input_length = max_text_length, mask_zero=True))\n",
    "    mt_model.add(LSTM(vec_length))\n",
    "    # Decoder\n",
    "    #Repetición del último output tantas veces como la longitud máxima de la lengua destino (eng_length).\n",
    "    #Fíjese que esta longitud máxima corresponde a estados de tiempo del output (out_timesteps)\n",
    "    #https://stackoverflow.com/questions/51749404/how-to-connect-lstm-layers-in-keras-repeatvector-or-return-sequence-true\n",
    "    mt_model.add(RepeatVector(out_timesteps))\n",
    "    #Añadir la capa LSTM con el mismo número de units que el LSTM del codificador. Establecemos que queremos\n",
    "    #obtener la secuencia completa del output; esto es, la traducción de la frase entera.\n",
    "    mt_model.add(LSTM(vec_length, return_sequences=True))\n",
    "    #Añadir la capa donde se representa la distribución de las palabras del vocabulario de la lengua destino \n",
    "    #según la probabilidad de aparición en la secuencia destino. Así podemos obtener la secuencia más probable\n",
    "    #de ser la traducción de la frase origen\n",
    "    mt_model.add(Dense(out_vocab_size, activation='softmax'))\n",
    "    return mt_model\n",
    "\n",
    "embedvl= 300\n",
    "model  = encodeco(engvsize, embedvl, seql, seql, deuvsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Compilació amb optimizador RMS\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "filename = 'model_eng_deu'\n",
    "\n",
    "#La función ModelCheckpoint() guarda el modelo con la pérdida de validación más baja.\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#Entrenamiento del modelo. Se realiza con 30 epoch con un tamaño de batch de 256, con un reparto de validación del\n",
    "#20%; esto es, 80% se destina al entrenamiento en sí, y el 20% restante a su validación.\n",
    "model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=30, batch_size=256,\n",
    "          validation_split=0.2, callbacks=[checkpoint], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8 199 199 ...   0   0   0]\n",
      " [  1  13  38 ...   0   0   0]\n",
      " [  1 195  15 ...   0   0   0]\n",
      " ...\n",
      " [  2   3  12 ...   0   0   0]\n",
      " [ 57  33   8 ...   0   0   0]\n",
      " [ 29  67  11 ...   0   0   0]]\n",
      "                           actual                   predicted\n",
      "100           stell das wasser ab             mach das wasser\n",
      "101           sie sind gesprächig          du bist gesprächig\n",
      "102         wer sind die typen da         wer ist diese leute\n",
      "103           was würde passieren               was würde das\n",
      "104                tom trägt blau                 tom hat den\n",
      "105            sie sind glücklich          sie sind glücklich\n",
      "106           eine frage habe ich         ich habe eine frage\n",
      "107           sprich die wahrheit      sagen sie die wahrheit\n",
      "108           das ist sehr salzig         das ist sehr dunkel\n",
      "109         tom wirkt eingebildet   tom scheint unzuverlässig\n",
      "110           ich muss tom warnen         ich muss tom finden\n",
      "111        tom muss zu hause sein         tom muss nach hause\n",
      "112   tom verschränkte seine arme  tom legte seine wasserhahn\n",
      "113          mach keine grimassen                   nicht sie\n",
      "114   er handelt auf eigene faust           er steht sich auf\n",
      "115  tom hat seine firma verkauft         tom ließ seine dame\n",
      "116                 weiter so tom                 geh und tom\n",
      "117          tom ist oft nicht da             tom ist mehr da\n",
      "118      schön dich wiederzusehen       ich werde dich  sehen\n",
      "119                  wo leben sie               wo wohnen sie\n"
     ]
    }
   ],
   "source": [
    "# Predicció\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model_eng_deu')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))\n",
    "print(preds)\n",
    "\n",
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        #Obtener la palabra que corresponde al índice del vocabulario de la lengua destino\n",
    "        t = get_word(i[j], deutok)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], deutok)) or (t == None):\n",
    "                     temp.append('')\n",
    "            else:\n",
    "                     temp.append(t)\n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "    preds_text.append(' '.join(temp).strip())\n",
    "\n",
    "pred_df = pd.DataFrame({'actual' : test[:,1], 'predicted' : preds_text})\n",
    "print(pred_df.iloc[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 TA amb Embeddings preentrenats (1 punt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest apartat es demana repetir l'exercici anterior carregant a la capa d'embedding els pesos d'un model Glove entrenat per l'anglès."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem el següent model GloVe per l'anglès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.42B.300d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació, hem de construir la matriu de embeddings.\n",
    "\n",
    "Per no carregar tot el vocabulari de el model, hem de filtrar només aquelles entrades presents en el vocabulari del tokenizer que farem servir. I a més, hem d'incloure en la matriu vectors corresponidientes als índexs de les entrades (paraules) que no trobem en el model glove carregat. Aquests vectors es solen inicialitzar com 0s o com a resultats d'una distribució N (0,1),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Per exemple, si el nostre tokenizer es diu `eng_tokenizer` podríem fer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(engtok.word_index)+1, 300))\n",
    "for word, i in engtok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Per iniciar una capa d'embedding amb pesos predefinits s'usa l'argument `weights`. A més, com no volem que es modifiquin els pesos vam marcar l'argument `trainable` com` False`.\n",
    "\n",
    "Seguint amb el nostre exemple faríem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(engtok.word_index)+1,\n",
    "                            embedvl,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=seql,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Implementació:</strong> \n",
    "Implementa i entrena de nou un model de traducció automàtica de l'anglès a l'alemany de forma similar, aquest cop carregant els pesos de la capa embedding a partir d'el model Glove preentrat en anglès disponible a 'glove.42B.300d.txt'.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓ                                   #\n",
    "#############################################\n",
    "def encodeco_glo(in_vocab_size, vec_length, max_text_length, out_timesteps, out_vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)    # Embedding layer amb pesos predefinits\n",
    "    model.add(LSTM(vec_length))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(vec_length, return_sequences=True))\n",
    "    model.add(Dense(out_vocab_size, activation='softmax'))\n",
    "    return(model)\n",
    "\n",
    "mod_glo= encodeco_glo(engvsize, embedvl, seql, seql, deuvsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "mod_glo.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "filename = 'model_eng_deu_glo'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "mod_glo.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), epochs=30, batch_size=256,\n",
    "            validation_split=0.2, callbacks=[checkpoint], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8 199  27 ...   0   0   0]\n",
      " [  1  16  18 ...   0   0   0]\n",
      " [  2 195  15 ...   0   0   0]\n",
      " ...\n",
      " [  2   3  12 ...   0   0   0]\n",
      " [ 57  33   8 ...   0   0   0]\n",
      " [ 29  67  11 ...   0   0   0]]\n",
      "                           actual              predicted\n",
      "100           stell das wasser ab    mach das wasser aus\n",
      "101           sie sind gesprächig                du bist\n",
      "102         wer sind die typen da   wer sind diese leute\n",
      "103           was würde passieren         was würde sich\n",
      "104                tom trägt blau         tom trägt blau\n",
      "105            sie sind glücklich     sie sind glücklich\n",
      "106           eine frage habe ich    ich habe eine frage\n",
      "107           sprich die wahrheit       sag sie wahrheit\n",
      "108           das ist sehr salzig           das ist sehr\n",
      "109         tom wirkt eingebildet      tom scheint nicht\n",
      "110           ich muss tom warnen     ich muss tom sagen\n",
      "111        tom muss zu hause sein       tom muss zu sein\n",
      "112   tom verschränkte seine arme      tom hob seine arm\n",
      "113          mach keine grimassen           machen keine\n",
      "114   er handelt auf eigene faust       er ist nicht auf\n",
      "115  tom hat seine firma verkauft  tom hat sein verkauft\n",
      "116                 weiter so tom          geh gehen tom\n",
      "117          tom ist oft nicht da    tom ist nicht lange\n",
      "118      schön dich wiederzusehen    ich freue mich dich\n",
      "119                  wo leben sie            wo sind sie\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model_eng_deu_glo')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))\n",
    "print(preds)\n",
    "\n",
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], deutok)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], deutok)) or (t == None):\n",
    "                     temp.append('')\n",
    "            else:\n",
    "                     temp.append(t)\n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "    preds_text.append(' '.join(temp).strip())\n",
    "\n",
    "pred_df = pd.DataFrame({'actual' : test[:,1], 'predicted' : preds_text})\n",
    "print(pred_df.iloc[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>(Opcional) Anàlisi:</strong> Explica quines són les principals diferències entre els dos models entrenats. Com podríem millorar els resultats d'aquesta tasca en concret?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tal com hem vist la diferència principal es troba en el fet de que la capa Embedding del model sigui supervisada o no. És a dir, si partim d'una matriu d'embedding aleatòria que haurà d'ajustar-se durant el entrenament (supervisada) o si comencem amb una matriu ja preentrenada amb capacitat d'extreure relacions semàntiques entre paraules.\n",
    "- Pel que fa als resultats no sembla haver-hi un clarament millor que l'altre. El model preentrenat triga lleugerament menys en executar cada época del entrenament, fet coherent en tant que els pesos són fixes.\n",
    "- Els punts que s'haurien de revisar a l'hora de considerar millores serien, per exemple, assegurar-nos de que el model supervisat tingui una quantitat de textos suficient i que el seu vocabulari sigui adient a l'estil del que volem traudir. També resultaria interessant fer servir tots dos métodes, generant la matriu d'embedding amb un preentrenament no-supervisat per després afinar els valors dels vectors durant l'entrenament de la xarxa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clasificació de frases (2 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest apartat plantegem l'ús de les arquitectures vistes fins al moment per millorar els resultats de la tasca de classificació d'opinions falses vista anteriorment en l'assignatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Primer carregarem les dades de el fitxer de titulars 'opinions-Tagged-FAKE.csv' amb l'objectiu d'entrenar un model que classifiqui les opinions a 'FAKENEG', 'FAKEPOS', 'TRUENEG' i 'TRUEPOS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('OPINIONS-TAGGED-FAKE.csv', sep='\\t')\n",
    "\n",
    "data = []\n",
    "data_labels = []\n",
    "\n",
    "opinions = df['OPINION'].tolist()\n",
    "tags = df['TAG'].tolist()\n",
    "\n",
    "for i in range(len(opinions)): \n",
    "    data.append(opinions[i]) \n",
    "    data_labels.append(tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Preparem i preprocessem les dades per a l'entrenament. Farem servir one-hot encoding per a les etiquetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKENEG' 'FAKENEG' 'FAKENEG' ... 'TRUEPOS' 'TRUEPOS' 'TRUEPOS']\n",
      "[0 0 0 ... 3 3 3]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "values = array(data_labels)\n",
    "print(values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La idea d'el model de classificació que volem implementar és més simple que la de l'encoder-decoder usat en l'apartat 1.\n",
    "El model ha de consistir només en:\n",
    "\n",
    "- una capa embedding amb els pesos de el model Glove preentrenat per a l'anglès disponible en el fitxer 'glove.42B.300d.txt'\n",
    "- una capa LSTM amb un nombre de units a triar (per exemple, 300)\n",
    "- una capa Donin amb dimensió de sortida el nombre de categories amb les que volem classificar (en aquest cas, 4).\n",
    "\n",
    "A més, com loss function `loss` farem servir 'categorical_crossentropy' i com` optimizer`, 'adam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong> Implementació:</strong> \n",
    "Adapta l'arquitectura vista en l'apartat anterior com s'indica a dalt per classificar els textos de opinions-Tagged-FAKED.csv en TRUENEG, FALSENEG, TRUEPOS, FALSEPOS. Compara els resultats amb els obtinguts en el Llibreta-PLA3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓ                                   #\n",
    "#############################################\n",
    "opin= [s.translate(str.maketrans('','',string.punctuation)).lower() for s in data]\n",
    "for i in range(len(opin)):\n",
    "    opin[i]= ''.join(opin[i]).split(' ')\n",
    "\n",
    "# Tokenització\n",
    "opitok= tokenization(opin)\n",
    "\n",
    "# Matriu d'embedding preentrenada\n",
    "embedding_matrix = np.zeros((len(opitok.word_index)+1, 300))\n",
    "for word, i in opitok.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Split train/test i codificació de les sequencies\n",
    "ml=0\n",
    "for s in opin:\n",
    "    if len(s)>ml:\n",
    "        ml=len(s)\n",
    "\n",
    "trainX, testX = train_test_split(opin, test_size=0.2, random_state=12)\n",
    "codtrain, codtest = train_test_split(onehot_encoded, test_size=0.2, random_state=12)\n",
    "\n",
    "opitrain= encode_sequences(opitok, ml, trainX)\n",
    "opitest = encode_sequences(opitok, ml, testX)\n",
    "\n",
    "# Definició i entrenament del model\n",
    "modfake = Sequential()\n",
    "modfake.add(Embedding(len(opitok.word_index)+1,\n",
    "                      embedvl,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=ml,\n",
    "                      trainable=False,\n",
    "                      mask_zero=True))\n",
    "modfake.add(LSTM(300))\n",
    "modfake.add(Dense(len(set(values)), activation='softmax'))\n",
    "\n",
    "filename = 'model_tag_fake'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "modfake.compile(optimizer='Adam', loss='categorical_crossentropy')\n",
    "#modfake.fit(opitrain, codtrain, epochs=30, batch_size=256, validation_split=0.2,\n",
    "#            callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67        67\n",
      "           1       0.79      0.81      0.80        77\n",
      "           2       0.70      0.54      0.61        90\n",
      "           3       0.76      0.71      0.73        86\n",
      "\n",
      "    accuracy                           0.70       320\n",
      "   macro avg       0.71      0.71      0.70       320\n",
      "weighted avg       0.71      0.70      0.70       320\n",
      "\n",
      "     actual  predicted\n",
      "187       3          0\n",
      "189       3          1\n",
      "79        1          1\n",
      "233       3          3\n",
      "132       0          0\n",
      "145       2          2\n",
      "135       1          1\n",
      "155       2          2\n",
      "242       2          0\n",
      "223       3          3\n",
      "278       3          1\n",
      "282       1          1\n",
      "111       1          1\n",
      "196       1          1\n",
      "188       2          2\n",
      "77        3          3\n",
      "172       3          3\n",
      "291       1          1\n",
      "262       1          1\n",
      "118       1          1\n"
     ]
    }
   ],
   "source": [
    "# Obtenció de mètriques\n",
    "from sklearn import metrics\n",
    "\n",
    "model = load_model('model_tag_fake')\n",
    "preds = model.predict_classes(opitest.reshape((opitest.shape[0],opitest.shape[1])))\n",
    "\n",
    "pred_df = pd.DataFrame({'actual' : np.argmax(codtest, axis=1), 'predicted' : preds})\n",
    "print(metrics.classification_report(pred_df['actual'], pred_df['predicted']))\n",
    "print(pred_df.sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong> Análisis:</strong>  \n",
    "Segons el que hem estudiat a teoria, què podríem fer per trobar un model millor?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pel que fa als resultats respecte a PLA3, podem veure que els d'aquesta arquitectura només són superiors als de Random Forest. És a dir, qualsevol model purament matemàtic amb un classificador basat SVM o Regressió Logística aconseguiex resultats superiors a la xarxa neuronal recursiva.\n",
    "- El resultat milloraria si el model fos capaç d'aplicar l'equivalent al sentit comú a les frases. Com a exemple tenim el model BERT, que permet fer una representació numèrica de les paraules segons el contexte en el que apareixen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta segona part de la pràctica es demana resoldre els exercici amb la llibreria **PYTORCH**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Anàlisi de sentiments (4 punts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "L'objectiu d'aquest apartat és entrenar un model d'anàlisi de sentiments per a un dataset de comentaris. El dataset consistirà en el camp `Short` de l'data frame de el fitxer 'tripadvisor_data.csv' i el sentiment el dóna el camp` Opinion`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong> Implementació :</strong> \n",
    "Entrena un model d'anàlisi de sentiment que usi embeddings entrenats amb BERT seguint els mateixos passos que en primer notebook (SA) de la lliçó 5, ia partir de les dades de l'dataset de comentaris de camp `Short` per al text i del camp `Opinion` per al sentiment .\n",
    "    \n",
    "Compara els resultats amb els obtinguts a la PRA2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Short  \\\n",
      "0                             “Very nice atmosphere”   \n",
      "1  “Very nice food, great atmosphere, feels like ...   \n",
      "2                         “Best Hotel on the Planet”   \n",
      "3                        “What a vacation should be”   \n",
      "4                                   “Excellent stay”   \n",
      "\n",
      "                                                Long    Class Opinion  \n",
      "0  We were together with some friends at the Anew...   family     POS  \n",
      "1  Martin and his staff are truely great! They ma...   family     POS  \n",
      "2  We have stayed at the Excelsior on numerous oc...   family     POS  \n",
      "3  Having four days free in Milan, we decided to ...  friends     POS  \n",
      "4  In all aspects an excellent stay. Professional...   couple     POS  \n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓ                                   #\n",
    "#############################################\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "df = pd.read_csv('tripadvisor_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model a partir d'embeddings BERT\n",
    "# Lectura i generació de TEXT i LABEL\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import random\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    maxlen = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:maxlen-2]\n",
    "    return tokens\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = tokenizer.cls_token_id,\n",
    "                  eos_token = tokenizer.sep_token_id,\n",
    "                  pad_token = tokenizer.pad_token_id,\n",
    "                  unk_token = tokenizer.unk_token_id)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [1523, 2190, 2155, 3309, 1012, 2053, 2342, 2000, 3231, 2500, 2065, 2115, 4268, 2024, 3920, 2084, 2184, 1013, 2340, 1012, 1012, 1012, 1524], 'label': 'POS'}\n",
      "Counter({'POS': 722, 'NEG': 155})\n"
     ]
    }
   ],
   "source": [
    "fields = [('text', TEXT),(None,None),(None,None),('label', LABEL)]\n",
    "\n",
    "train_data = data.TabularDataset(path = 'tripadvisor_data.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True)\n",
    "\n",
    "train_data, valid_data, test_data = train_data.split(split_ratio = [0.6, 0.2, 0.2], random_state = random.seed(SEED))\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.freqs)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definició del model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        with torch.no_grad(): # así congelamos los parámetros de BERT durante en entrenamiento\n",
    "            embedded = bert(text)[0]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 112,241,409 parámetros\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model     = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "#Número de parámetros\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'El modelo tiene {count_parameters(model):,} parámetros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenament del model\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.509 | Train Acc: 82.26%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 79.06%\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.395 | Train Acc: 83.71%\n",
      "\t Val. Loss: 0.439 |  Val. Acc: 81.63%\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.310 | Train Acc: 88.56%\n",
      "\t Val. Loss: 0.473 |  Val. Acc: 85.17%\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.275 | Train Acc: 89.40%\n",
      "\t Val. Loss: 0.448 |  Val. Acc: 85.42%\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.246 | Train Acc: 91.31%\n",
      "\t Val. Loss: 0.473 |  Val. Acc: 84.79%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    corrections = (rounded_preds == y).float()\n",
    "    acc = corrections.sum() / len(corrections)\n",
    "    return acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.96      0.91       238\n",
      "         1.0       0.68      0.39      0.49        54\n",
      "\n",
      "    accuracy                           0.85       292\n",
      "   macro avg       0.78      0.67      0.70       292\n",
      "weighted avg       0.84      0.85      0.84       292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modificamos la función para que devuelva las predicciones\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluatemod(model, iterator, criterion):\n",
    "    predictions_all = []\n",
    "    labels_all = []\n",
    "    \n",
    "    model.eval()  # deshabilita dropout y batch normalization\n",
    "    \n",
    "    with torch.no_grad(): # para no calcular los gradientes durante las computaciones\n",
    "    \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            predictions_all +=  torch.round(torch.sigmoid(predictions)).flatten().cpu().numpy().tolist()\n",
    "            labels_all += batch.label.flatten().cpu().numpy().tolist()\n",
    "        \n",
    "    return predictions_all, labels_all\n",
    "\n",
    "predictions, labels = evaluatemod(model, test_iterator, criterion)\n",
    "print(metrics.classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podem veure que en aquest cas el resultat és similar al millor model obtingut en la PRA2, el TfIdf amb classificador SVM amb una accuracy de 0.89 i alta precisió a tots dos valors objectiu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación binaria de textos (1 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong> Implementació :</strong> \n",
    "Entenent la tasca d'anàlisi de sentiments com una classificació binari (p.ex. en 0 i 1), reutilitza la mateixa arquitectura per entrenar ara un classificador binari que classifiqui els titulars de camp 'Headline' a 'NYT-Comment-Headlines.csv' segons el camp 'Tag' a TOP i NOTOP.\n",
    "    \n",
    "Compara els resultats amb els obtinguts en la PRA1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [2062, 6677, 2084, 18250, 1010, 1998, 2210, 3930, 2013, 9230], 'label': 'NONTOP'}\n",
      "Counter({'NONTOP': 578, 'TOP': 253})\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓ                                   #\n",
    "#############################################\n",
    "fields = [(None,None),('text', TEXT),('label', LABEL)]\n",
    "\n",
    "train_data = data.TabularDataset(path = 'NYT-Comment-Headlines.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True)\n",
    "\n",
    "train_data, valid_data, test_data = train_data.split(split_ratio = [0.6, 0.2, 0.2], random_state = random.seed(SEED))\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.freqs)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.675 | Train Acc: 64.01%\n",
      "\t Val. Loss: 0.575 |  Val. Acc: 72.44%\n",
      "Epoch: 02 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.604 | Train Acc: 70.16%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 73.07%\n",
      "Epoch: 03 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.535 | Train Acc: 74.00%\n",
      "\t Val. Loss: 0.554 |  Val. Acc: 74.94%\n",
      "Epoch: 04 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.476 | Train Acc: 79.06%\n",
      "\t Val. Loss: 0.551 |  Val. Acc: 75.57%\n",
      "Epoch: 05 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 0.435 | Train Acc: 80.98%\n",
      "\t Val. Loss: 0.560 |  Val. Acc: 74.63%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut7-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.92      0.83       199\n",
      "         1.0       0.56      0.24      0.34        78\n",
      "\n",
      "    accuracy                           0.73       277\n",
      "   macro avg       0.66      0.58      0.59       277\n",
      "weighted avg       0.70      0.73      0.69       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, labels = evaluatemod(model, test_iterator, criterion)\n",
    "print(metrics.classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El resultat és superior al Tfidf amb regressió lineal generat a la PRA1, el qual retornava un 0.71 de accuracy amb una precisió molt baixa pel que fa a la classificació de titulars TOP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Anàlisi de sentiments per aspectes (1 punt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En aquest apartat aplicarem els coneixements estudiats en la part d'ABSA del mòdul 5 i els combinarem amb altres coneixements per analitzar comentaris de tripadvisor sobre hotels.\n",
    "\n",
    "Primer carreguem les dades per analitzar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tripadvisor_data = pd.read_csv('tripadvisor_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A continuació carreguem un model d'anàlisi de sentiments per aspectes de Targeted ABSA.\n",
    "L'etiqueta 0 correspon a negatiu, 1, a neutre i 2, a positiu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_absa(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (dense): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "BERT_DIM = 768\n",
    "DROPOUT = 0.1\n",
    "POLARITIES_DIM = 3\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class BERT_absa(nn.Module):\n",
    "    def __init__(self, bert, dropout, bert_dim, polarities_dim):\n",
    "        super(BERT_absa, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dense = nn.Linear(bert_dim, polarities_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_bert_indices, bert_segments_ids = inputs.text, inputs.term\n",
    "        _, pooled_output = self.bert(text_bert_indices, bert_segments_ids)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.dense(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "modelba = BERT_absa(bert, DROPOUT, BERT_DIM, POLARITIES_DIM)\n",
    "\n",
    "modelba.load_state_dict(torch.load('bert_absa'))\n",
    "modelba.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong> Implementació :</strong> \n",
    "Selecciona almenys dues parelles de `frases-terme` usant elements de camp 'Short' com` frase` i termes de la mateixa frase que siguin aspectes com a `terme`.\n",
    "    \n",
    "Troba la predicció de el model carregat per el sentiment d'aquest terme en aquesta frase. <br>\n",
    "Notes: <br>\n",
    "   - Pots ajudar-te dels resultats de la PRA2 per buscar frases. <br>\n",
    "   - Pensa que també pots cercar la polaritat de diferents aspectes dins de la mateixa frase, pàg. ex. en la frase 'nice location but overpriced and poor service', hi ha dos aspectes: location i service. <br>\n",
    "   - El model carregat ha estat entrenat amb una seqüència màxima de tokens de 80. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anàlisi de sentiments per aspectes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  frase     terme  label\n",
      "1     “Very nice food, great atmosphere, feels like ...  location      2\n",
      "357                       “Overbooked and poor servive”  location      1\n",
      "443                   “Lovely hotel fantastic location”  location      2\n",
      "1098                                          “Just ok”  location      1\n",
      "1034                   “Excellent pension and location”  location      2\n",
      "289                  “Great location, hotel and owners”  location      2\n",
      "939                          “Nice and quiet location.”  location      2\n",
      "1367  “Very friendy family run hotel with great pool...  location      1\n",
      "400                          “You get what you pay for”  location      1\n",
      "200   “A great little hotel in an fantastic Italian ...  location      1\n",
      "1070                       “Great value, good location”  location      2\n",
      "333                    “Rooms small but great location”  location      2\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓ                                   #\n",
    "#############################################\n",
    "df = pd.read_csv('tripadvisor_data.csv')\n",
    "#print(df.sample(20))\n",
    "dfabsa= pd.DataFrame({'frase': df.iloc[[1,357,443,1098,1034,289,939,1367,400,200,1070,333]]['Short']})\n",
    "dfabsa['terme']=['location']*12\n",
    "dfabsa['label']=[2,1,2,1,2,2,2,1,1,1,2,2]\n",
    "dfabsa.to_csv('dfabsa.csv')\n",
    "print(dfabsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': array([  101,  1523,  2200,  2767,  2100,  2155,  2448,  3309,  2007,\n",
      "        2307,  4770,  1998, 12403,  4128,  1524,   102,  3295,   102,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0],\n",
      "      dtype=int64), 'term': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64), 'polarity': '1'}\n",
      "Counter({'2': 4, '1': 3})\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(sequential = False, use_vocab=False)\n",
    "TERM = data.Field(sequential = False, use_vocab=False)\n",
    "POLARITY = data.LabelField()\n",
    "\n",
    "fields = [(None,None), ('text',TEXT), ('term',TERM), ('polarity',POLARITY)] \n",
    "\n",
    "train_data = data.TabularDataset(path = 'dfabsa.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = fields,\n",
    "                                        skip_header = True)\n",
    "\n",
    "train_data, test_data = train_data.split(split_ratio = [0.6, 0.4], random_state = random.seed(SEED))\n",
    "\n",
    "def tokenizar_e_indexar_con_bert(dataset, tokenizer):\n",
    "    maxlen = 80\n",
    "    for i in range(len(dataset.examples)):\n",
    "        \n",
    "        text = dataset.examples[i].text\n",
    "        term = dataset.examples[i].term\n",
    "        text_raw_sequence = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "        term_raw_sequence = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(term))\n",
    "        \n",
    "\n",
    "        if len(term_raw_sequence)>=int(maxlen/4):\n",
    "            term_raw_sequence = term_raw_sequence[0:int(maxlen/4)]\n",
    "        seq = text_raw_sequence[:maxlen-3-len(term_raw_sequence)]\n",
    "\n",
    "        text_bert_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"])\\\n",
    "            + seq +  tokenizer.convert_tokens_to_ids([\"[SEP]\"])\\\n",
    "            + term_raw_sequence + tokenizer.convert_tokens_to_ids([\"[SEP]\"])\n",
    "\n",
    "        bert_segments_ids = [0] * (len(seq)+2) + [1] * (len(term_raw_sequence) + 1) + [0]*(maxlen-len(text_bert_ids))\n",
    "\n",
    "        text_bert_ids = text_bert_ids + [0]*(maxlen-len(text_bert_ids))\n",
    "        new_text = [text_bert_ids, bert_segments_ids]\n",
    "        \n",
    "        dataset.examples[i].text = np.asarray(text_bert_ids, dtype='int64')\n",
    "        dataset.examples[i].term = np.asarray(bert_segments_ids, dtype='int64')\n",
    "\n",
    "tokenizar_e_indexar_con_bert(train_data, tokenizer)\n",
    "tokenizar_e_indexar_con_bert(test_data, tokenizer)\n",
    "\n",
    "print(vars(train_data.examples[0]))\n",
    "POLARITY.build_vocab(train_data)\n",
    "print(POLARITY.vocab.freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "# si tenemos GPU disponible se computarán ahí los datos\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=False,\n",
    "    device = device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() \n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()  \n",
    "        predictions = model(batch)   \n",
    "        loss = criterion(predictions, batch.polarity.reshape(-1).long())        \n",
    "        acc = categorical_accuracy(predictions, batch.polarity.reshape(-1).long())     \n",
    "        loss.backward()     \n",
    "        optimizer.step()      \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval() \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "    \n",
    "        for batch in iterator:\n",
    "            predictions = model(batch)     \n",
    "            loss = criterion(predictions, batch.polarity.reshape(-1).long())\n",
    "            acc = categorical_accuracy(predictions, batch.polarity.reshape(-1).long())\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "modelba   = modelba.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(modelba.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.764 | Train Acc: 28.57%\n",
      "\t Test Loss: 0.755 |  Test Acc: 60.00%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.084 | Train Acc: 57.14%\n",
      "\t Test Loss: 1.166 |  Test Acc: 40.00%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.342 | Train Acc: 28.57%\n",
      "\t Test Loss: 2.670 |  Test Acc: 40.00%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.458 | Train Acc: 42.86%\n",
      "\t Test Loss: 0.740 |  Test Acc: 60.00%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.988 | Train Acc: 42.86%\n",
      "\t Test Loss: 0.793 |  Test Acc: 40.00%\n"
     ]
    }
   ],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y) # check if it is correct\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(modelba, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(modelba, test_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'absa.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Test Loss: {valid_loss:.3f} |  Test Acc: {valid_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
